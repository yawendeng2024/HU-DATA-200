{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7656922f-97f7-4dcc-be62-faf5041b0b76",
   "metadata": {},
   "source": [
    "# Setting up our Code Environment\n",
    "Congratulations on getting your Integrated Development Environment (IDE) installed. Let's walk through some of the basics of your environment. Jupyter Notebook and the cloud-based Jupyter Lab are block editors. Many block editors are available, and Jupyter Notebook is just one. Feel free to use the one that you are most familiar with. There are two main types of blocks we will use. <br>\n",
    "\n",
    "## Introduction to Jupyter Notebook\n",
    "The **Markdown** block allows a block of text and images to be placed in the Notebook. This is essential to your work because it allows the work to be kept in line with the observations of the results. Using this method, you will not have to create a separate document or capture screenshots of the code. <br>\n",
    "\n",
    "* Markdown accepts HTML code to format the text. The `<br>` is a manual line break in markdown.\n",
    "* The asterisk begins an unordered list similar to the `<ul>` in HTML.\n",
    "* The `#` is used to create headers. A single `#` is the largest, and adding multiple `#` symbols creates nested headers within the larger header. This helps you navigate to a specific section using the Table of Contents.\n",
    "\n",
    "The **Code** block is a block that allows you multiple lines to generate your code. This is read as code, and any text placed here without the # symbol will create an error in the code. The `#` symbol allows you to add a single line of markdown text to your code. It is essential to comment on what you are doing, and it can help direct your thoughts for a process requiring multiple lines of code. <br>\n",
    "\n",
    "As you can see above, we have designated our code kernel as Python 3. You can change this to other versions of Python or another language. JupyterLab also codes in R. Other Anaconda Navigator programs code in Julia and even in C++ or Ruby.\n",
    "\n",
    "The **Docustring** is markdown text placed in a code block that spans several lines. It is usually used to convey longer text describing what a process will do or the expected outcome. Docustrings are traditionally added to a user-defined function to document what the processes are intended to do. To define the start of the docstring, we use three single quotes in a row, type the information in the docstring, and then close with three single quotes. <br>\n",
    "\n",
    "You can also embed images in your Markdown text if needed to illustrate a point. For instance, if you were taking notes in the Markdown fields and wanted to embed a screenshot of a slide from our lecture, you could do that. I will create a separate document to illustrate image embedding for review offline.\n",
    "\n",
    "### Text suggestions in Jupyter\n",
    "One very useful feature is the predictive text feature of Jupyter Notebook and Jupyter Lab. When working on a new method, you can type the start of the method name and press the tab; this will offer suggestions for methods that match what you have typed. When you hover, with your mouse, over the method name, it will give you additional information from the developer documents that include the arguments and keyword arguments (args and kwargs) for the method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6a05d-0591-403e-b6ae-37a00041da5b",
   "metadata": {},
   "source": [
    "Several packages are needed that will be used throughout our work in this course. This notebook will serve as the environment setup. You will only need to run this once, but when saved, it can generate the environment on a new installation. Let's start with pip installs of numpy, pandas, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6753be72-2a7d-420b-aa2e-2c6622faa20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This is the beginning of the docustring\n",
    "This function will perform the desired action.\n",
    "The expected outcome is...'''\n",
    "x = 7\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc498c6d-cb90-49c2-8b0f-b0e0a6d1789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d81dfc-9232-47d6-a2bc-34d0764ef009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Applications/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1291156-17f9-4da0-9f8e-76e155644713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5b91a-7819-4da8-a477-11acd823bacc",
   "metadata": {},
   "source": [
    "## Developing a Research Question\n",
    "The Data Science process begins with a robust research question.  This topic should be exciting and incorporate domain knowledge from your expertise. The research question should be relevant to a historical or current event that prompts a study or requires further inquiry.\n",
    "\n",
    "- Do some preliminary research on your topic to learn more about it. This will help you narrow your focus and identify potential research questions.\n",
    "- Narrow your topic down to a specific area you want to investigate. This will help to make your research question more manageable and feasible.\n",
    "- Formulate several research questions that you could answer about your topic. Be sure to ask open-ended questions that cannot be answered with a simple yes or no.\n",
    "- Evaluate your research questions to make sure that they are:\n",
    "  - Specific: They should be focused on a specific aspect of your topic.\n",
    "  - Researchable: There should be enough information available to answer them.\n",
    "  - Feasible: They should be possible to answer within the scope of your research project.\n",
    "  - Interesting: They should be something that you are motivated to answer.\n",
    "\n",
    "Choose one or two research questions to focus on for your project. Revise your questions as needed as you learn more about your topic.\n",
    "\n",
    "Here are some additional tips for developing a robust research question:\n",
    "\n",
    "- Start with a question that you are genuinely curious about. This will make your research more enjoyable and productive.\n",
    "- Be sure to consider your audience when formulating your research question. Who are you writing for? What do they need to know?\n",
    "- Use clear and concise language in your research question. Ensure your question reaches a broad audience of multiple  levels of understanding. Avoid jargon and technical terms that your audience may not understand.\n",
    "- Be sure to define your terms. What do you mean by the key terms in your research question?\n",
    "Use a variety of sources to inform your research question. This will help you gain a well-rounded perspective on your topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9b755-7c64-4fd6-babc-687ce62f036d",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "**Environmental Racism** includes the intentional pollution of the environments where predominately Black, Indigenous, and other People of Color (BIPOC) communities reside. These communities tend to be impoverished or low-income communities, frequently redlined by local, state, or federal governments. These communities are often further marginalized with lower wages resulting from undereducation from schools with low ratings or low instructor retention rates.\n",
    "<br>\n",
    "***\n",
    "<br>\n",
    "\n",
    "1. <font color=#003A63>*In your role as a member of the Data Team please formulate a research question that discusses **environmental racism** and the effect on the surrounding communities.*</font>\n",
    "\n",
    "2. <font color=#003A63>*Explore and curate at least one data set to support your research question. Examples of some datasets about **environmental racism** are below*.\n",
    "\n",
    "> Link to  __[Envrionmental Racism Datasets](https://colab.research.google.com/drive/17usNiFPJ1jilIsn2RE9NN7FmWSKgbN7Z#scrollTo=LH-F6wzIak6f)__</font>\n",
    "\n",
    "3. <font color=#003A63>*Cleaning the dataset. This is a critical step in the analysis of the data. Explore the size of the data, the type of data present, look for gaps in the data and missing values that will affect the analysis of the data. Determine the next steps to ensure the data can be analyzed.*</font>\n",
    "\n",
    "4. <font color=#003A63>*Analyze the dataset. Data Scientists use their knowledge of statistical methods to analyze the data and identify relationships between the variables. This step allows the team to determine the best way to prepare the data for a machine learning algorithm and select the appropriate model(s) to fully explore the research question.*</font>\n",
    "\n",
    "5. <font color=#003A63>*Interpret and report the results of the data analysis and/or machine learning models. Communicate the results using charts and figures as well as words to convey the findings to other researchers, policymakers, and business partners. Be sure to identify the implications of the findings for future research studies.*</font>\n",
    "\n",
    "6. <font color=#003A63>*Scientific research is reproducible. This means that the techniques used and the models created should be able to be used by other researchers to derive the same or similar results. This adds a layer of transparency to the research findings and permits peer review, identifying any gaps in the conclusions as well as celebrating the impact of the research performed. Additional team members and those outside of the team should be able to run the code produced and arrive at similar results. To ensure the same results for machine learning models consider the use of Random State which generates the same partition of training, validation, and test data splits as opposed to randomly generated splits which would yield similar but different results for consecutive runs.*</font>\n",
    "\n",
    "#### Example of a research topic\n",
    "Environmental racism as a topic can geolocate communities of color stratified by racial identification and ethnicity compared to the locations where pollution concentration is highest. This would need to define pollution as a topical argument, define the racial and ethnic groups comprising the BIPOC Community, and the criteria for pollution levels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a822703-04ae-4873-a4c2-43f433451f4d",
   "metadata": {},
   "source": [
    "## Potential subtopics\n",
    "* The intersectionality of public health disparities, such as reactive airway disorders and air quality ratings.\n",
    "* The frequency of unmanageable or untreatable asthma, chronic obstructive pulmonary disorder (COPD), and measured particulate matter in air.\n",
    "* Geolocation of measured particulate matter, ozone, nitrogen dioxide, and sulfur dioxide in communities of color.\n",
    "* Location of Superfund Sites and occurrences of childhood cancer within a fifty-mile radius.\n",
    "* Spatial study of cancer clusters by diagnosis type within a radius of Superfund Sites.\n",
    "* Longitudinal study of groundwater contamination from mining wastewater.\n",
    "* Heavy mineral and metals poisoning of surface and groundwater from mining sludge and wastewater.\n",
    "* The sources and effects of heavy mineral pollution of soil and agricultural commodities for animal consumption.\n",
    "*  The relative effects on aquatic life and native niche plants from lithium mining used for electric vehicle batteries.\n",
    "* Water consumption for extinguishing thermal runaway fires from lithium-ion batteries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3e1a8-add5-4c18-92a5-7e1082c9f58d",
   "metadata": {},
   "source": [
    "## Data Definition\n",
    "\n",
    "* **Air Quality Measures on the National Environmental Health Tracking Network.** <br>\n",
    "Last Updated: July 20, 2023. <br>\n",
    "https://catalog.data.gov/dataset/air-quality-measures-on-the-national-environmental-health-tracking-network<br>\n",
    "This dataset combines the Environmental Protection Agency (EPA) Air Quality System (AQS) database containing data from approximately 4,000 monitoring stations around the country, mainly in urban areas. Data from the AQS is considered the \"gold standard\" for determining outdoor air pollution. Centers for Disease Control and Prevention (CDC) and EPA have worked together to develop a statistical model (Downscaler) to make modeled predictions available for environmental public health tracking purposes in areas of the country that do not have monitors and to fill in the time gaps when monitors may not be recording data.\n",
    "\n",
    "* **Global Fire Emissions Database, Version 4.1 (GFEDv4)** <br>\n",
    "Last Updated: July 27, 2023 <br>\n",
    "https://catalog.data.gov/dataset/global-fire-emissions-database-version-4-1-gfedv4 <br>\n",
    "This dataset provides global estimates of monthly burned area, monthly emissions and fractional contributions of different fire types. National Aeronautics and Space Administration (NASA) emissions data are available for carbon (C), dry matter (DM), carbon dioxide (CO2), carbon monoxide (CO), methane (CH4), hydrogen (H2), nitrous oxide (N2O), nitrogen oxides (NOx), non-methane hydrocarbons (NMHC), organic carbon (OC), black carbon (BC), particulate matter less than 2.5 microns (PM2.5), total particulate matter (TPM), and sulfur dioxide (SO2) among others. These data are yearly totals by region, globally, and by fire source for each region.\n",
    "\n",
    "* **PM2.5 and cardiovascular mortality rate data: Trends modified by county socioeconomic status in 2,132 US counties** <br>\n",
    "Last Updated: November 12, 2020 <br>\n",
    "https://catalog.data.gov/dataset/annual-pm2-5-and-cardiovascular-mortality-rate-data-trends-modified-by-county-socioeconomi <br>\n",
    "U.S. Environmental Protection Agency Data on county socioeconomic status for 2,132 US counties and each county’s average annual cardiovascular mortality rate (CMR) and total PM2.5 concentration for 21 years (1990-2010). County CMR, PM2.5, and socioeconomic data were obtained from the U.S. National Center for Health Statistics, U.S. Environmental Protection Agency’s Community Multiscale Air Quality modeling system, and the U.S. Census, respectively.\n",
    "\n",
    "* **Superfund Site Information**<br>\n",
    "Last Updated: May 17, 2021<br>\n",
    "https://catalog.data.gov/dataset/superfund-site-information <br>\n",
    "U.S. Environmental Protection Agency asset includes a number of individual data sets related to site-specific information for Superfund, which contains basic site description, location, schedule of activities, enforcement and settlement data, contaminants and selected remedy and much more, as well as the records that clearly document site decisions. This asset also includes sampling data and lab results (CLPSS, EDDs), redevelopment and technical assistance case studies, site reuse and land revitalization information, EPAOSC.net information, Superfund Technical Assistance Grants information, site management information records (RODs, Remediation plans, cleanup directives), contract management information, and more.\n",
    "\n",
    "* **Superfund cleanups and children’s lead exposure in six states** <br>\n",
    "Last Updated: July 26, 2021 <br>\n",
    "https://catalog.data.gov/dataset/superfund-cleanups-and-childrens-lead-exposure-in-six-states <br>\n",
    "Data for the study include restricted access and non-restricted access files. Restricted access files include individual children's blood lead data from six states, property assessment data from Zillow, Inc., and Census tract characteristics processed by GeoLytics. This dataset includes contaminated site locations and characteristics (Superfund, brownfields, and RCRA sites), ambient air lead concentrations, state-month average temperatures, and vehicle miles traveled in 1980.\n",
    "\n",
    "* **EPA Region 6 REAP Sustainability Geodatabase** <br>\n",
    "Last Updated: November 10, 2020 <br>\n",
    "https://catalog.data.gov/dataset/epa-region-6-reap-sustainability-geodatabase <br>\n",
    "The Regional Ecological Assessment Protocol (REAP) is a screening level assessment tool created as a way to identify priority ecological resources within the five EPA Region 6 states (Arkansas, Louisiana, New Mexico, Oklahoma, and Texas). The REAP divides eighteen individual measures into three main sub-layers: diversity, rarity, and sustainability. This geodatabase contains the 2 grids (sustain and sustainrank) representing the sustainability layer which describes the state of the environment in terms of stability (sustainble areas are those that can maintain themselves into the future without human management). There are eleven measures that make up the sustainability layer: contiguous land cover, regularity of ecosystem boundary, appropriateness of land cover, waterway obstruction, road density, airport noise, Superfund sites, Resource Conservation and Recovery Act (RCRA) sites, water quality, air quality, and urban/agriculture disturbance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62d712-4006-44d7-9d1a-650f3a528fb3",
   "metadata": {},
   "source": [
    "# Data Science Process\n",
    "The data science process consists of seven key steps. Each week, we will complete another step of the process. The work we complete in class each week will structure the work you will complete on the group project. This week you will work on working with your dataset file, loading data into the python environment, and making a plan for data cleaning steps. <br>\n",
    "\n",
    "There are seven main steps in the data science process. The steps you take will vary depending on the specific problem you are trying to solve. However, the general process will be the same.\n",
    "1. Problem framing: This is the first and most important step in the data science process. It involves understanding the research question that you are trying to solve and defining the specific questions that you want to answer with data.\n",
    "2. Data Collection: Once you have defined your problem, you need to acquire the data you need to answer your questions. This can involve collecting data from various sources, such as surveys, databases, and social media.\n",
    "3. Data Cleaning: Once you have acquired your data, you must prepare it for analysis. This involves cleaning the data, removing errors and outliers, and formatting it so that it is easy to work with.\n",
    "4. Data Exploration: This step involves exploring your data to understand it better. This includes looking at summary statistics, creating visualizations, and asking questions about the data.\n",
    "5. Modeling: This step involves building models to predict or explain the data. You can use many different types of models, such as linear regression, logistic regression, and decision trees.\n",
    "6. Evaluation: Once you have built your models, you must evaluate them to see how well they perform. This involves using metrics such as accuracy, precision, and recall to measure their performance.\n",
    "7. Deployment: Once you have found a model that performs well, you need to deploy it so that it can be used to make predictions or decisions. This can involve creating a web application, a mobile app, or a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32081029-e0ad-4030-8baa-3e7c9b659e98",
   "metadata": {},
   "source": [
    "## Data Collection:\n",
    "Data collection begins with identifying a reliable and accurate data source and using tools to download the dataset for examination. Next, the necessary libraries are imported, which contain pre-written code that performs specific tasks. Python has several libraries, which are robust data analysis and visualization tools.\n",
    "\n",
    "Once the dataset is loaded and the libraries imported, the dataset can be read, and the dataframe can be created. Now, the data is checked, and the data cleaning process begins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
